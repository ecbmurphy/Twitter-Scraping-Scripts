{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235f31bb-217b-44a2-808f-2147a5bb8c1a",
   "metadata": {},
   "source": [
    "## Set up the environment and import key libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaff7640-00de-47f0-bc0e-175c4e69e12e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import jsonlines\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b109934-272c-4215-8941-ce2ecb23d4cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define API Endpoint & OAuth Bearer Token:\n",
    "The API endpoint is the url the script will send its requests to, that Twitter's servers are listening on (defined by \"endpoint\"). It is currently set to access Version 2 of their Academic API.\n",
    "\n",
    "\"bearer_token\" defines the confidential security token generated by Twitter, which it uses to keep track of and authorize API requests - it is what lets Twitter know that the requests are coming from Dr. Murphy's account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fccd3e9-d7dc-49c3-8387-6cf03aee2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAACC9lQEAAAAAyYkInf7JgCsCy8W%2BqMHo8oV65Ms%3DoWBU0XvLnHMd8Pg9ZJprIsbY3gulAVazGQ55pikR1DhmNUsNY0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6107022-9ebc-479e-8b29-84949b5a9cb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Defining our query:\n",
    "\"Query\" can be any combination of search operators accepted by twitter's Full-Archive Search API (as found at https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query).\n",
    "\n",
    "\"start_time\" and \"end_time\" define the time period from which you want to pull your search results. Must be formatted in UTC (ie. 2023-01-01T00\\:00:00Z).\n",
    "\n",
    "\"max_tweets\" sets a cap on the maximum number of results that will be returned. Can be set to whatever you want, bearing in mind the 10,000,000 tweet/month limit of the Academic API and that relatively narrow/obscure searches may not even reach the value you set.\n",
    "\n",
    "\"max_results\" defines the maximum number of tweets that will be returned per individual request of the API. Best left to 500, as pagination is on and all returned tweets will be concatenated into the same \"dataset.jsonl\" file upon output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776c2766-3eca-49ec-90be-92735f11ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '(\"Spot\" (\"Boston Dynamics\" OR \"Robot\")) OR (\"Atlas\" (\"Boston Dynamics\" OR \"Robot\")) OR \"BigDog\" OR \"Roomba\" OR \"iRobot\" OR \"ASIMO\" OR \"iCub\" OR (\"valkyrie\" (\"NASA\" OR \"robot\")) OR (\"Spirit\" (\"Mars\" OR \"NASA\" OR \"science\" OR \"rover\")) OR (\"Opportunity\" (\"Mars\" OR \"NASA\" OR \"science\" OR \"rover\")) OR (\"Curiosity\" (\"Mars\" OR \"NASA\" OR \"science\" OR \"rover\")) OR (\"Voyager\" (\"1\" OR \"2\" OR \"PROBE\" OR \"NASA\" OR \"space\" OR \"probe\"))'\n",
    "\n",
    "start_time = \"2006-03-22T00:00:00Z\"\n",
    "end_time = \"2023-04-01T00:00:00Z\"\n",
    "\n",
    "max_tweets = 100000\n",
    "max_results = 500\n",
    "\n",
    "filepath = \"I:\\\\Nextcloud\\\\Twitter Scraping Pilot Project\\\\Jupyter Notebooks\\\\2wttr\\\\Production\\\\Broad Scope Topic Search\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef08331-4fc1-4d88-9143-d4c20938be68",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Defining API parameters.\n",
    "We define the different types of parameters we might send to the API, and the parameters themselves. These tell the API what types of data to send back to us, and how those data fields are categorized.\n",
    "\n",
    "The type \"query\" only has one type of parameter - \"query\" - which we defined in the last cell. It forms the backbone of our request.\n",
    "\n",
    "The type \"expansions\" has several parameters:\n",
    "   - \"author_id\": the Twitter User ID of the tweet's author.\n",
    "   - \"referenced_tweets.id\": the Twitter ID of any referenced/quoted tweets from the tweet.\n",
    "   - \"geo.place_id\": the Twitter ID of any places/locations specifically named in the tweet.\n",
    "   - \"in_reply_to_user_id\": the Twitter User ID of the author of the tweet being replied to.\n",
    "   - \"referenced_tweets.id.author_id\":\n",
    "    \n",
    "The type \"tweet.fields\" has several parameters:\n",
    "   - \"created_at\": date/time the tweet was created.\n",
    "   - \"author_id\": same as \"author_id\" above.\n",
    "   - \"lang\": language of the tweet.\n",
    "   - \"entities\":\n",
    "   - \"geo\":\n",
    "   - \"referenced_tweets\": text/content of the tweet being replied to.\n",
    "   - \"in_reply_to_user_id\": same as \"in_reply_to_user_id\" above.\n",
    "   - \"public_metrics\": numbers of likes, retweets & replies.\n",
    "\n",
    "The type \"user.fields\" only has one parameter, \"username\" - the username of the account that created the tweet.\n",
    "\n",
    "The types \"start_time\", \"end_time\", and \"max_results\" each only have one parameter, named the same as their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5243cf-920d-4143-bbb9-2704b78d385a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'query': query,\n",
    "    'expansions': 'author_id,referenced_tweets.id,geo.place_id,in_reply_to_user_id,referenced_tweets.id.author_id',\n",
    "    'tweet.fields': 'created_at,author_id,lang,entities,geo,referenced_tweets,in_reply_to_user_id,public_metrics', \n",
    "    'user.fields': 'username',\n",
    "    'start_time': start_time,\n",
    "    'end_time': end_time,\n",
    "    'max_results': max_results,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb4d00-e43c-411c-b53f-e3910d254939",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Define API request header & attaches bearer token authorization to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "add45f1a-b750-498d-89db-55baba3562f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98e77b-74ff-4be9-8809-d200488c5285",
   "metadata": {},
   "source": [
    "# Initialize empty list in which to store returned tweets, until query is completed and they are written out to \"dataset.jsonl\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "342e553f-8d3e-43c3-8d62-55c5b54755e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [] # initialize list of tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c4a040-c508-40b6-9737-8976e046a4e3",
   "metadata": {},
   "source": [
    "# Define a function to fetch tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b2a419-08e2-4ad7-96e3-a69a13a5d391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_tweets():\n",
    "    response = requests.request(\"GET\", endpoint, headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    json_response = response.json()\n",
    "    return json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ddb28-331a-47ed-9c10-5d2d8475cab0",
   "metadata": {},
   "source": [
    "# Define a function to parse the incoming tweets from the Twitter API's JSON response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f1ab8ef-79bd-4cdd-8607-14b75764ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tweets(json_response):\n",
    "    if len(json_response['data']) == 0:\n",
    "        return None\n",
    "\n",
    "    user_dict = extract_user_data(json_response)\n",
    "    tweets = []\n",
    "    for tweet_dict in json_response['data']:\n",
    "        tweet_dict = add_user_data_to_tweet(user_dict, tweet_dict)\n",
    "        tweets.append(tweet_dict)\n",
    "        if len(tweets) == max_tweets:  # Use max_tweets from outside the function\n",
    "            break\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa9856-63a7-4ea5-971e-9f5f3a8a9144",
   "metadata": {},
   "source": [
    "# Define a function to extract the data about each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23405b93-939e-44b9-83fa-1edd30e29ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_user_data(json_response):\n",
    "    user_dict = {}\n",
    "    for user_data in json_response['includes']['users']:\n",
    "        user_id = user_data['id']\n",
    "        user_dict[user_id] = {'username': user_data['username'], 'name': user_data['name']}\n",
    "    return user_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7019be5f-4cb5-464c-ac4d-42fb5805ce9b",
   "metadata": {},
   "source": [
    "# Define a function to add the extracted data to the appropriate tweet object in the temporary dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35ea4d9f-ee8e-45f2-a4f9-c5959d45e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_user_data_to_tweet(user_dict, tweet_dict):\n",
    "    user_id = tweet_dict['author_id']\n",
    "    if user_id in user_dict:\n",
    "        user_data = user_dict[user_id]\n",
    "        tweet_dict['username'] = user_data['username']\n",
    "        tweet_dict['name'] = user_data['name']\n",
    "    return tweet_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dee144-9058-4255-9ea5-d56faa10ec04",
   "metadata": {},
   "source": [
    "# Define a function to paginate tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80bb2372-a2a2-4572-973b-60034c54c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paginate_tweets(json_response):\n",
    "    if 'next_token' not in json_response['meta']:\n",
    "        return None\n",
    "    else:\n",
    "        next_token = json_response['meta']['next_token'] # get next_token\n",
    "        params['pagination_token'] = next_token # add pagination key to query dict\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d2ec9a-d225-4e0d-aa50-c0fe0a14b53c",
   "metadata": {},
   "source": [
    "# Define a function to create an appropriate file name and strip out any illegal windows characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e14d5d0-3e42-4b84-a675-571adfb4e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filename(query):\n",
    "    # Create a filename based on the query and current date/time\n",
    "    now = datetime.now().strftime('%Y-%m-%d, %H_%M')\n",
    "    filename = f\"{query} - {now}.jsonl\"\n",
    "    \n",
    "    # Replace invalid characters in filename with [symbol name]\n",
    "    invalid_chars = {\n",
    "        \"<\": \"LT\",\n",
    "        \">\": \"GT\",\n",
    "        \":\": \"COL\",\n",
    "        \"\\\"\": \"QOUT\",\n",
    "        \"/\": \"FS\",\n",
    "        \"\\\\\": \"BS\",\n",
    "        \"|\": \"VB\",\n",
    "        \"?\": \"QUES\",\n",
    "        \"*\": \"AS\"\n",
    "    }\n",
    "    for char, name in invalid_chars.items():\n",
    "        filename = filename.replace(char, f\"[{name}]\")\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d6ffbb-9801-4909-88a8-62260de5aec8",
   "metadata": {},
   "source": [
    "# Define a function to write each tweet out into a .jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeb23e98-102e-4cda-a712-a6c5e447f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweets_to_file(tweets, filepath, query):\n",
    "    # Create a filename based on the query and current date/time\n",
    "    #filename = create_filename(query)\n",
    "    filename = f\"realspecific.jsonl\"\n",
    "    \n",
    "    # Check if filepath is valid, otherwise fall back to default directory\n",
    "    if not os.path.isdir(filepath):\n",
    "        print(f\"Invalid directory path {filepath}. Defaulting to current directory.\")\n",
    "        filepath = \".\"\n",
    "    \n",
    "    # Write tweets to file\n",
    "    file = os.path.join(filepath, filename)\n",
    "    with jsonlines.open(file, 'w') as writer:\n",
    "        for tweet in tweets:\n",
    "            writer.write(tweet)\n",
    "    \n",
    "    print(f\"Saved {len(tweets)} tweets to {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378a98c-9a8c-47c3-ac7f-9d3ee479e78d",
   "metadata": {},
   "source": [
    "# Fetch and parse tweets until max_tweets reached or no more tweets to fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e9207a2-7d60-4268-8709-ef4e069bb604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n",
      "Getting tweets\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "(429, '{\"title\":\"Too Many Requests\",\"detail\":\"Too Many Requests\",\"type\":\"about:blank\",\"status\":429}')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tweets) \u001b[38;5;241m<\u001b[39m max_tweets:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetting tweets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     json_response \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     parsed_tweets \u001b[38;5;241m=\u001b[39m parse_tweets(json_response)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parsed_tweets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mfetch_tweets\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, endpoint, headers\u001b[38;5;241m=\u001b[39mheaders, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m      5\u001b[0m json_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json_response\n",
      "\u001b[1;31mException\u001b[0m: (429, '{\"title\":\"Too Many Requests\",\"detail\":\"Too Many Requests\",\"type\":\"about:blank\",\"status\":429}')"
     ]
    }
   ],
   "source": [
    "while len(tweets) < max_tweets:\n",
    "    print(\"Getting tweets\")\n",
    "    json_response = fetch_tweets()\n",
    "\n",
    "    parsed_tweets = parse_tweets(json_response)\n",
    "    if parsed_tweets is None:\n",
    "        break # end loop if no more tweets returned\n",
    "\n",
    "    tweets += parsed_tweets\n",
    "\n",
    "    if len(tweets) == max_tweets:\n",
    "        break # end loop if max number of tweets have been added\n",
    "\n",
    "    paginated = paginate_tweets(json_response)\n",
    "    if paginated is None:\n",
    "        break # end loop if no more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b02643-c840-4d01-b43d-48a5643aa625",
   "metadata": {
    "tags": []
   },
   "source": [
    "# We print \"done\" to the console to indicate that the requests have finished, & save the .jsonl file of tweets to a specified location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec0eed2-e1c0-4948-902d-322ed623ba57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Done\")\n",
    "save_tweets_to_file(tweets, filepath, query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
